{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7da6df7",
   "metadata": {},
   "source": [
    "SoftwareHeritage Data Extraction script (based on keywords in readme + authors email address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import RetryError, Timeout\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# timeout handling\n",
    "def safe_request(method, url, session, max_attempts=5, timeout=15, **kwargs):\n",
    "    \"\"\"\n",
    "    Performs session.request(method, url) with:\n",
    "      - honoring Retry-After on HTTP 429, but on the second consecutive 429\n",
    "        pauses for 1 hour before retrying\n",
    "      - exponential backoff on timeouts / network errors (up to max_attempts)\n",
    "      - raises after max_attempts timeouts / network errors\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    rate_limit_hits = 0\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            resp = session.request(method, url, timeout=timeout, **kwargs)\n",
    "        except Timeout:\n",
    "            if attempt >= max_attempts:\n",
    "                raise\n",
    "            wait = 2 ** (attempt - 1)\n",
    "            print(f\"[Timeout] {method} {url} (attempt {attempt}/{max_attempts}), retrying in {wait}s…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        except RetryError:\n",
    "            if attempt >= max_attempts:\n",
    "                raise\n",
    "            wait = 2 ** (attempt - 1)\n",
    "            print(f\"[Network error] {method} {url} (attempt {attempt}/{max_attempts}), retrying in {wait}s…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        if resp.status_code == 429:\n",
    "            rate_limit_hits += 1\n",
    "            if rate_limit_hits >= 2:\n",
    "                print(f\"[429] {method} {url} – second rate-limit hit, sleeping for 1 hour…\")\n",
    "                time.sleep(3600)\n",
    "                rate_limit_hits = 0\n",
    "            else:\n",
    "                ra = resp.headers.get(\"Retry-After\", \"\")\n",
    "                wait = int(ra) if ra.isdigit() else 60\n",
    "                print(f\"[429] {method} {url} – retrying after {wait}s…\")\n",
    "                time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        rate_limit_hits = 0\n",
    "        resp.raise_for_status()\n",
    "        return resp\n",
    "\n",
    "# Configuration\n",
    "KEYWORDS         = [\"unibo\", \"unibo.it\", \"alma mater\", \"alma mater studiorum\",\n",
    "                    \"university of bologna\", \"almamaterstudiorum\",\n",
    "                    \"università di bologna\", \"universita di bologna\"]\n",
    "API_BASE         = \"https://archive.softwareheritage.org/api/1\"\n",
    "SEARCH_PARAMS    = {\"with_visit\": \"true\", \"limit\": 1000}\n",
    "CACHE_FILE       = \"cached_candidate_origins.json\"\n",
    "PROCESSED_FILE   = \"processed_origins.json\"\n",
    "CONFIRMED_FILE   = \"unibo_repositories_swh.json\"\n",
    "DEFAULT_TIMEOUT  = 15\n",
    "LOG_LIMIT        = 200  # page size for log pagination\n",
    "\n",
    "# Prepare HTTP session\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"Accept\": \"application/json\"\n",
    "})\n",
    "adapter = HTTPAdapter(max_retries=Retry(\n",
    "    total=3, backoff_factor=0.5,\n",
    "    status_forcelist=[500,502,503,504],\n",
    "    allowed_methods=[\"GET\",\"POST\"]\n",
    "))\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "# 1. Load or fetch candidate origins\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        origins = json.load(f)\n",
    "    print(f\"Loaded {len(origins)} origins from cache.\")\n",
    "else:\n",
    "    all_entries = []\n",
    "    for kw in KEYWORDS:\n",
    "        print(f\"Searching '{kw}'…\")\n",
    "        resp = safe_request(\"GET\", f\"{API_BASE}/origin/search/{kw}/\",\n",
    "                            session, timeout=DEFAULT_TIMEOUT, params=SEARCH_PARAMS)\n",
    "        all_entries.extend(resp.json())\n",
    "        while 'Link' in resp.headers:\n",
    "            links = requests.utils.parse_header_links(\n",
    "                resp.headers['Link'].rstrip('>').replace('>,','>,')\n",
    "            )\n",
    "            nxt = next((l['url'] for l in links if l.get('rel') == 'next'), None)\n",
    "            if not nxt:\n",
    "                break\n",
    "            resp = safe_request(\"GET\", nxt, session, timeout=DEFAULT_TIMEOUT)\n",
    "            all_entries.extend(resp.json())\n",
    "    unique = {e['url']: e for e in all_entries if isinstance(e, dict) and 'url' in e}\n",
    "    origins = list(unique.values())\n",
    "    print(f\"Found {len(origins)} unique origins; caching…\")\n",
    "    with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(origins, f, indent=2)\n",
    "\n",
    "# fetch complete revision history with automatic pagination\n",
    "import requests.utils\n",
    "\n",
    "def fetch_revision_log(rev):\n",
    "    logs = []\n",
    "    params = {\"limit\": LOG_LIMIT}\n",
    "    url = f\"{API_BASE}/revision/{rev}/log/\"\n",
    "\n",
    "    while url:\n",
    "        resp = safe_request(\"GET\", url, session, timeout=DEFAULT_TIMEOUT, params=params)\n",
    "        data = resp.json()\n",
    "        logs.extend(data)\n",
    "        link_hdr = resp.headers.get(\"Link\", \"\")\n",
    "        next_url = None\n",
    "        if link_hdr:\n",
    "            links = requests.utils.parse_header_links(\n",
    "                link_hdr.rstrip('>').replace('>,','>,')\n",
    "            )\n",
    "            next_url = next((l['url'] for l in links if l.get('rel') == 'next'), None)\n",
    "        url = next_url\n",
    "        params = None\n",
    "    return logs\n",
    "\n",
    "# 2. Filter and confirm UNIBO repos with resume support\n",
    "last_idx = -1\n",
    "if os.path.exists(PROCESSED_FILE):\n",
    "    with open(PROCESSED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        last_idx = json.load(f).get(\"last_index\", -1)\n",
    "confirmed = []\n",
    "if os.path.exists(CONFIRMED_FILE):\n",
    "    with open(CONFIRMED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        confirmed = json.load(f)\n",
    "\n",
    "for idx, entry in enumerate(origins):\n",
    "    if idx <= last_idx:\n",
    "        continue\n",
    "    url = entry.get('url')\n",
    "    print(f\"\\n[{idx}/{len(origins)}] Checking {url}\")\n",
    "\n",
    "    if not url:\n",
    "        continue\n",
    "\n",
    "    # 2.1 Latest snapshot\n",
    "    lv = safe_request(\"GET\", f\"{API_BASE}/origin/{url}/visit/latest/\", session,\n",
    "                      timeout=DEFAULT_TIMEOUT, params={\"require_snapshot\": \"true\"}).json()\n",
    "    snap_id = lv.get(\"snapshot\")\n",
    "    if not snap_id:\n",
    "        print(\" → no snapshot, skip\")\n",
    "        continue\n",
    "\n",
    "    # 2.2 Snapshot branches\n",
    "    snap = safe_request(\"GET\", f\"{API_BASE}/snapshot/{snap_id}/\", session,\n",
    "                        timeout=DEFAULT_TIMEOUT).json()\n",
    "    branches = snap.get(\"branches\", {})\n",
    "    if not branches:\n",
    "        print(\" → no branches, skip\")\n",
    "        continue\n",
    "    if 'refs/heads/master' in branches:\n",
    "        br = branches['refs/heads/master']\n",
    "    elif 'refs/heads/main' in branches:\n",
    "        br = branches['refs/heads/main']\n",
    "    else:\n",
    "        br = next(iter(branches.values()))\n",
    "    if br.get(\"target_type\") != \"revision\":\n",
    "        print(\" → branch not revision, skip\")\n",
    "        continue\n",
    "    rev = br[\"target\"]\n",
    "\n",
    "    # 2.3 Revision metadata\n",
    "    rev_meta = safe_request(\"GET\", f\"{API_BASE}/revision/{rev}/\", session,\n",
    "                            timeout=DEFAULT_TIMEOUT).json()\n",
    "    dir_id = rev_meta.get(\"directory\")\n",
    "\n",
    "    # 2.4 Directory listing\n",
    "    entries = safe_request(\"GET\", f\"{API_BASE}/revision/{rev}/directory/\", session,\n",
    "                           timeout=DEFAULT_TIMEOUT).json().get(\"content\", [])\n",
    "\n",
    "    # 2.5 README check\n",
    "    readme_ok = False\n",
    "    for e in entries:\n",
    "        if e.get(\"type\") == \"file\" and e.get(\"name\", \"\").lower().startswith(\"readme\"):\n",
    "            blob = e.get(\"target\")\n",
    "            txt = safe_request(\"GET\",\n",
    "                f\"{API_BASE}/content/sha1_git:{blob}/raw/\", session,\n",
    "                timeout=DEFAULT_TIMEOUT).text.lower()\n",
    "            if any(k in txt for k in [\"unibo.it\", \"università di bologna\", \"university of bologna\", \"alma mater studiorum\"]):\n",
    "                readme_ok = True\n",
    "            break\n",
    "\n",
    "    # 2.6 Bulk log for authors\n",
    "    try:\n",
    "        log = fetch_revision_log(rev)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error fetching log] {e} — skip repo\")\n",
    "        continue\n",
    "    authors = set()\n",
    "    author_ok = False\n",
    "    for role in (\"author\", \"committer\"):\n",
    "        a = rev_meta.get(role, {}) or {}\n",
    "        name = (a.get(\"name\") or a.get(\"fullname\") or \"\").strip()\n",
    "        email = (a.get(\"email\") or \"\").strip()\n",
    "        if name or email:\n",
    "            authors.add((name, email))\n",
    "            if \"unibo.it\" in email.lower():\n",
    "                author_ok = True\n",
    "    for rec in log:\n",
    "        for role in (\"author\", \"committer\"):\n",
    "            a = rec.get(role, {}) or {}\n",
    "            name = (a.get(\"name\") or a.get(\"fullname\") or \"\").strip()\n",
    "            email = (a.get(\"email\") or \"\").strip()\n",
    "            if name or email:\n",
    "                authors.add((name, email))\n",
    "                if \"unibo.it\" in email.lower():\n",
    "                    author_ok = True\n",
    "\n",
    "    if readme_ok or author_ok:\n",
    "        auth_list = [{\"name\": n, \"email\": e} for (n, e) in sorted(authors)]\n",
    "        confirmed.append({\"url\": url, \"rev\": rev, \"dir_id\": dir_id, \"authors\": auth_list})\n",
    "        print(f\" → confirmed UNIBO repo; authors: {auth_list}\")\n",
    "        with open(CONFIRMED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(confirmed, f, indent=2)\n",
    "    else:\n",
    "        print(\" → not UNIBO, skip\")\n",
    "\n",
    "    # write checkpoint ONLY after fully processing item\n",
    "    with open(PROCESSED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": idx}, f)\n",
    "\n",
    "print(f\"\\nFiltering done: {len(confirmed)} confirmed repos.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
